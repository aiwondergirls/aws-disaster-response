# -*- coding: utf-8 -*-
"""XGBoost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TphxpybiUhl7BE-2c5kryz8ylPCUBNdP

We will use an XGBoost Regressor model that predicts the '**_Total Affected_**' population based on the magnitude, depth, GDP, rural, and urban population, poulation density and other attributes.

**Data** 

We used EM-DAT(https://www.emdat.be/guidelines), US geological survey and World bank data to build our dataset. 

We combined the datasets based on geographic location.

The Features in the dataset are as follows:
CPI -Corruption Perception Index (CPI), 
depth - depth of the earthquake
mag - magnitude of the earthquake(Richter Scale)
mmi - The modified Mercalli intensity scale (MM or MMI), developed from Giuseppe Mercalli's Mercalli intensity scale of 1902, is a seismic intensity scale used for measuring the intensity of shaking produced by an earthquake

Population density (people per sq. km of land area)

Population growth (annual %)

Population in largest city

Population in the largest city (% of urban population)

Population, total

Rural population

Rural population (% of total population) 

Urban population

Urban population (% of total population)

Urban population growth (annual %)

GDP (constant 2015 US$)

GDP (current US$)

GDP growth (annual %)

GDP per capita (constant 2015 US$)

country

Total Affected - The total number of people affected is the sum of injured, affected and homeless in a disaster

## Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np   
import pandas as pd    
import matplotlib.pyplot as plt 
# %matplotlib inline 
import seaborn as sns
import sklearn.metrics as metrics
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error

df = pd.read_csv('df_merged_model.csv')
# Dataset is now stored in a Pandas Dataframe

"""## Load and review data"""

df.head()

#Rows and Columns
print(df.shape)
print(df.columns)

df.describe().T

df.country.unique()

df.loc[df["mag"]>8.0]  #There are 2 earthquakes that were >8.0 on the Richter scale, in Chile and Indonesia

df.drop_duplicates() #dropping duplicates, if any

import seaborn as sns; sns.set_theme(color_codes=True)
#tips = sns.load_dataset("tips")
ax = sns.scatterplot(x="mag", y="Total Affected", data=df)

"""Higher magnitude earthquakes dont always translate to mroe people affected."""

sns.boxplot(x=df['mag']);  #there are a few earthquakes with very high magnitude(outliers), that have occured, we wont be removing them

sns.boxplot(x=df['mmi'])

sns.boxplot(x=df['depth'])

df[df.depth> 50.0].depth.count()  # there are 81 data points with earthquake depth larger than 50m

d50 = df[df.depth> 50.0]

import seaborn as sns; sns.set_theme(color_codes=True)
#tips = sns.load_dataset("tips")
ax = sns.scatterplot(x="mag", y="Total Affected", data=d50)

"""Deeper Earthquakes, affect less folks, than shallower earthquakes, barring a few data points"""

#Plotting timeseries with respect to Type to geta better understanding
plt.figure(figsize=(12,8))
aff_pop=sns.lineplot(x=df['depth'],y="Total Affected",data=df)
aff_pop.set_title("Affected Population versus Depth of Earthquake")
aff_pop.set_ylabel("Population", color="#58508d")
aff_pop.set_xlabel("Depth", color="#58508d")

"""# Starting with a subset of features(consulting domain experts) """

data = df[['depth', 'mag', 'mmi',
       'Population density (people per sq. km of land area)',       'Rural population (% of total population)', 
       'Urban population (% of total population)',  'GDP growth (annual %)', 'Total Affected']]

data.info()

data.isna().sum()

data.loc[:,'GDP growth (annual %)'].replace('..', np.NaN, inplace=True)  # there are some rows with '..' , replacing with NAN
data.loc[:,'Population density (people per sq. km of land area)'].replace('..', np.NaN, inplace=True)

data["GDP growth (annual %)"] = data["GDP growth (annual %)"].astype(str).astype(float)  #converting object type to float
data["Population density (people per sq. km of land area)"] = data["Population density (people per sq. km of land area)"].astype(str).astype(float)

data.head()

data.isna().sum()

data.dropna(inplace=True)  #Dropping all data rows with missing values

print(data.shape)
data.head()

data.describe().T

sns.boxplot(x=data['Total Affected']);

data.loc[data["Total Affected"]>1000000.0].count()  # 55 earthquakes affected more than a miilion folks

"""# Preparing Data for the Model"""

data.columns

# Separating Target Variable from the dataset 
y = data["Total Affected"]
X = data.iloc[:,:]
X.drop("Total Affected", axis = 1, inplace = True)

"""### Split Data"""

#Train test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 22)

#Scaling the data
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X.columns

"""## Fit XGBoost Regressor Model"""

# For XGBoost
import xgboost as xgb
from xgboost.sklearn import XGBRegressor
from sklearn.model_selection import GridSearchCV #Perforing grid search

#Dmatrix
dtrain = xgb.DMatrix(X_train_scaled, label=y_train)
dtest = xgb.DMatrix(X_test_scaled, label=y_test)

xgb_model = XGBRegressor(nthread=-1) 
parameters = {
              'objective':["reg:squarederror"],
              'learning_rate': [.03, 0.05, .07], #so called `eta` value
              'max_depth': [5, 6, 7],
              'min_child_weight': [4, 5, 6,7],
              'silent': [1],
              'subsample': [0.5, 0.6, 0.7, 0.8],
              'colsample_bytree': [0.3, 0.5, 0.7],
              'n_estimators': [500]}

xgb_grid = GridSearchCV(xgb_model,
                        parameters,
                        cv = 5,
                        n_jobs = 5,
                        verbose=True)

xgb_grid.fit(X_train_scaled, y_train)

print(xgb_grid.best_score_)
print(xgb_grid.best_params_)

new_params = {"objective":"reg:squarederror", 'subsample':0.8,'colsample_bytree': 0.3,'learning_rate': 0.07,
                'max_depth': 7, 'min_child_weight': 4, 'alpha': 10,'n_estimators': 500}

xgb_best = xgb.train(
    new_params,
    dtrain,
    num_boost_round=999,
    evals=[(dtest, "Test")],
        early_stopping_rounds=10
)

# Testing
y_pred_xgb = pd.DataFrame( { "actual": y_test, 
"predicted": xgb_best.predict(dtest )  } ) 
y_pred_xgb

import pickle

xgb_best.save_model('finalxgboost2.model')

!pip install shap

import shap

explainer = shap.TreeExplainer(xgb_best)
shap_values = explainer.shap_values(X_test_scaled)

shap.summary_plot(shap_values, X_test,color='red', plot_type="bar")

# train XGBoost model

model = xgb.train({"learning_rate": 0.07}, dtrain, 100)
shap.initjs()
# explain the model's predictions using SHAP

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X)

# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)
shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:])

# Summarize the effects of all the features
shap.summary_plot(shap_values, X)